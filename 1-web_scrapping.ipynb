{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code cell, is an all_inclusive function that access the website, retrieve the information needed and stores them into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscrape_data (universe, nb_pages):\n",
    "    #To initialize the webpage number to 1\n",
    "    page = 1\n",
    "\n",
    "    #To create an empty list in which I will add all the characters webpage\n",
    "    all_pages = []\n",
    "\n",
    "\n",
    "    #While loop to nb_pages which can be found on the website itself\n",
    "    while page < nb_pages:\n",
    "        website_page = f\"https://www.superherodb.com/dc-comics/{universe}/?page_nr={page}\"\n",
    "\n",
    "        #To ensure the request of each page is granted or denied\n",
    "        req =  requests.get(website_page)\n",
    "        if req.status_code == 200:\n",
    "            pass\n",
    "        else:\n",
    "            print('request has been denied, try again')\n",
    "    \n",
    "        #To use beautifulsoup on the request\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "        #To find all the class needed\n",
    "        results = soup.findAll('div', class_= 'column col-12')[2]\n",
    "\n",
    "        #To set a variable to my results\n",
    "        results = results.select('a[href]')\n",
    "\n",
    "        #To create a variable with all the characters webpage\n",
    "        all_pages.extend(results)\n",
    "\n",
    "        #To iterate through the while loop\n",
    "        page += 1   \n",
    "\n",
    "    #To create an empty dataframe for the universe characters\n",
    "    universe_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    #To identify the website\n",
    "    website = 'https://www.superherodb.com'\n",
    "\n",
    "    #To create an empty dataframe to store the names\n",
    "    df_names = pd.DataFrame()\n",
    "\n",
    "    #To create an empty dataframe to store the powerstats\n",
    "    df_powerstats = pd.DataFrame()\n",
    "\n",
    "\n",
    "    #This for loop extracts all names and powerstats from each characters\n",
    "    #1- Names\n",
    "    for page in all_pages:\n",
    "\n",
    "        #To send a request\n",
    "        req = requests.get(website+page['href'])\n",
    "\n",
    "        #To use beautifulsoup for readability purposes\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "        #To locate the names\n",
    "        all_names = soup.find('div', class_ = 'columns profile-titles')\n",
    "\n",
    "        #To find the names\n",
    "        names = all_names.find(class_ = 'column col-10 col-md-9 col-sm-12').h1.get_text()\n",
    "\n",
    "        #To add names into df_names\n",
    "        df_names= df_names.append([names], ignore_index=True)\n",
    "\n",
    "\n",
    "    #2- Powerstats\n",
    "        #To find all the stats in the class needed\n",
    "        all_stats = soup.find('div', class_= 'stat-bar')\n",
    "\n",
    "        #In this particular website, the stats where located under scripts, the following line simply fetches the location in a text format\n",
    "        all_stats = all_stats.parent.find_next_sibling('script').get_text()\n",
    "\n",
    "        #Furthermore, the stats are located in a subcategory: \"bars\", this step fetches the stats under bars and splits its content and retain the first element\n",
    "        #Lastly it is turned into a dictionary\n",
    "        powerstats = json.loads(all_stats.split('\\\"bars\\\":')[1].split(\",\\\"shdbclass\")[0])\n",
    "\n",
    "        #To add powerstats into df_powerstats\n",
    "        df_powerstats = df_powerstats.append(powerstats, ignore_index=True)\n",
    "\n",
    "\n",
    "    #To merge all dataframes\n",
    "    #first merge names and powerstats dataframes\n",
    "    universe_df = pd.merge(df_names, df_powerstats, left_index = True, right_index = True)\n",
    "\n",
    "    #To change column names\n",
    "    universe_df.columns = [['Names', 'Intelligence', 'Strength', 'Speed', 'Durability', 'Power', 'Combat', 'Tier']]\n",
    "\n",
    "    return (universe_df)\n",
    "\n",
    "\n",
    "#In this particular website, the DC and Marvel url are deffined by the codes below\n",
    "DC  = '70-2'\n",
    "Marvel = '70-1'\n",
    "\n",
    "#To export the data into a CSV file\n",
    "webscrape_data(DC, 92).to_csv('DC_data')\n",
    "webscrape_data(Marvel, 103).to_csv('Marvel_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def battles(nb_pages):\n",
    "\n",
    "    #To initialize iterable variables\n",
    "    page = 1\n",
    "    counter = 0\n",
    "    counter2 = 1\n",
    "\n",
    "    #To initialize lists\n",
    "    P1 = []\n",
    "    P2 = []\n",
    "    win1 = []\n",
    "    draw = []\n",
    "    win2 = []\n",
    "\n",
    "    while page < nb_pages:\n",
    "        website_page = f\"https://www.superherodb.com/filler/90-{page}/\"\n",
    "\n",
    "        #To ensure the request of each page is granted or denied\n",
    "        req =  requests.get(website_page)\n",
    "        if req.status_code == 200:\n",
    "            pass\n",
    "        else:\n",
    "            print('request has been denied, try again')\n",
    "\n",
    "        #To use beautifulsoup for readability purposes\n",
    "        soup = BeautifulSoup(requests.get(website_page).content)\n",
    "\n",
    "        #check if page contains info\n",
    "        #stat results\n",
    "        battle_results = soup.find(\"div\", class_ = \"battle-result\")\n",
    "        if not battle_results:\n",
    "            #to skip this page as it doesn't contain results\n",
    "            continue\n",
    "\n",
    "        #Battle results\n",
    "        #P1 = 0, P2 = 1\n",
    "        Person = soup.find(\"h1\", class_ = \"h1-battle\").get_text().split(\"vs\")\n",
    "\n",
    "        battle_results = battle_results.find_all(\"div\")\n",
    "        P1_win = battle_results[0].get_text().split(\"(\")[1].split(\")\")[0].rstrip(\"%\")\n",
    "\n",
    "        draws = battle_results[1].get_text().split(\"(\")[1].split(\")\")[0].rstrip(\"%\")\n",
    "\n",
    "        P2_win = battle_results[2].get_text().split(\"(\")[1].split(\")\")[0].rstrip(\"%\")\n",
    "\n",
    "        if P1_win:\n",
    "            P1.append(Person[0])\n",
    "            P2.append(Person[1])\n",
    "            win1.append(P1_win)\n",
    "            draw.append(draws)\n",
    "            win2.append(P2_win)\n",
    "            counter +=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if counter >= 1000 or page == 772194:\n",
    "            df = pd.DataFrame(list(zip(P1,P2,win1,draw,win2)),columns=['Name1', 'Name2', 'Name1_res', 'Draw', 'Name2_res'])\n",
    "            #do df to csv here\n",
    "            df.to_csv(f\"battle_results_{counter2}.csv\", index =False)\n",
    "            print(f\"Reached battle iteration: {page}\")\n",
    "            counter2 += 1\n",
    "            counter = 0\n",
    "            P1 = []\n",
    "            P2 = []\n",
    "            win1 = []\n",
    "            draw = []\n",
    "            win2 = []\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "#nb of page is 772194 as of 16May2022, users can post new battles\n",
    "battles(772194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84430998ba504e8dffa029ad9fc2f03fc039634a6f12ea7e80284dea5fe9ffdb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('final_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
